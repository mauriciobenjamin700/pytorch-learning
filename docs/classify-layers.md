# ğŸ§  MODELO MENTAL: CADA CAMADA Ã‰ UM CONTRATO

Para **toda camada**, vocÃª deve saber:

1. **O que ela recebe**
2. **De quem ela recebe**
3. **O que ela faz**
4. **O que ela devolve**
5. **Para quem ela devolve**
6. **O que NÃƒO Ã© responsabilidade dela**

Vamos passar camada por camada **numa CNN de classificaÃ§Ã£o**, sem pular nada.

---

## ğŸ“¥ 1. ENTRADA (Input)

### âœ” ENTRADA Recebe

* Um tensor: `torch.Tensor`
* Shape: `(batch, channels, height, width)`
  * Ex: `(32, 3, 32, 32)`

#### ENTRADA Legenda

| DimensÃ£o | Significado               |
| -------- | ------------------------- |
| `batch`  | Quantas imagens           |
| `channels`   | Quantos canais de entrada |
| `H`      | Altura                    |
| `W`      | Largura                   |

Imagem RGB â†’ C_in = 3

Imagem em escala de cinza â†’ C_in = 1

Feature map intermediÃ¡rio â†’ C_in = nÂº de filtros anteriores

### âœ”ENTRADA Quem envia

* `DataLoader`

### âŒ ENTRADA NÃ£o faz

* NÃ£o aprende
* NÃ£o normaliza
* NÃ£o classifica

---

## ğŸ”² 2. Conv2d â€” *Extrator de padrÃµes locais*

### âœ” Conv2d Recebe

```python
(batch, C_in, H, W)
```

#### Conv2d Legenda

| DimensÃ£o | Significado               |
| -------- | ------------------------- |
| `batch`  | Quantas imagens           |
| `C_in`   | Quantos canais de entrada |
| `H`      | Altura                    |
| `W`      | Largura                   |

### âœ” Conv2d Recebe de

* Entrada da rede **ou**
* SaÃ­da de outra Conv2d

### âœ” Conv2d Faz

* Aplica filtros convolucionais
* Detecta padrÃµes **locais**
* Aprende pesos (kernels)

### âœ” Conv2d Retorna

```python
(batch, C_out, H_out, W_out)
```

| DimensÃ£o | Significado             |
| -------- | ----------------------- |
| `batch`  | Mesmo lote              |
| `C_out`  | NÂº de filtros da Conv   |
| `H_out`  | Altura apÃ³s convoluÃ§Ã£o  |
| `W_out`  | Largura apÃ³s convoluÃ§Ã£o |

Cada filtro gera 1 mapa de ativaÃ§Ã£o

C_out = quantos padrÃµes diferentes a camada aprende

```bash
nn.Conv2d(3, 32, 3)
```

```bash
â†’ C_out = 32
```

#### Conv2d Legenda Retorno

| DimensÃ£o | Significado               |
| -------- | ------------------------- |
| `batch`  | Quantas imagens           |
| `C_out`   | Quantos canais de entrada |
| `H`      | Altura                    |
| `W`      | Largura                   |

### âœ” Conv2d Retorna para

* AtivaÃ§Ã£o (ReLU)
* NormalizaÃ§Ã£o
* Outra Conv

### âŒ Conv2d NÃƒO faz

* Decidir classe
* Reduzir batch
* Garantir nÃ£o-linearidade

ğŸ“Œ **Regra mental**:

> Conv sÃ³ se preocupa com **caracterÃ­sticas espaciais**

---

## âš¡ 3. ReLU â€” *Decisor de ativaÃ§Ã£o*

### âœ” ReLU Recebe

```python
(batch, C, H, W)
```

### âœ” ReLU Recebe de

* Conv
* Linear

### âœ” ReLU Faz

* Zera valores negativos
* Introduz **nÃ£o-linearidade**

### âœ” ReLU Retorna

```python
(batch, C, H, W)
```

### âœ” ReLU Retorna para

* Pooling
* Outra Conv
* Linear

### âŒ ReLU NÃƒO faz

* Aprender pesos
* Extrair padrÃµes
* Normalizar dados

ğŸ“Œ **Regra mental**:

> ReLU sÃ³ decide **o que passa e o que morre**

---

## ğŸ“‰ 4. Pooling â€” *Redutor de resoluÃ§Ã£o*

### âœ” Pooling Recebe

```python
(batch, C, H, W)
```

### âœ” Pooling Recebe de

* ReLU

### âœ” Pooling Faz

* Reduz resoluÃ§Ã£o espacial
* MantÃ©m informaÃ§Ã£o dominante

### âœ” Pooling Retorna

```python
(batch, C, H/2, W/2)
```

| DimensÃ£o | MudanÃ§a          |
| -------- | ---------------- |
| `batch`  | Igual            |
| `C`      | Igual            |
| `H/2`    | Altura reduzida  |
| `W/2`    | Largura reduzida |

NÃ£o cria novos padrÃµes

SÃ³ reduz a resoluÃ§Ã£o

### âœ” Pooling Retorna para

* Outra Conv

### âŒ Pooling NÃƒO faz

* Aprender
* Classificar
* Mudar nÃºmero de canais

ğŸ“Œ **Regra mental**:

> Pool reduz *onde* a informaÃ§Ã£o estÃ¡, nÃ£o *o que* ela Ã©

---

## ğŸ” BLOCO COMPLETO (conv â†’ relu â†’ pool)

ğŸ‘‰ **Responsabilidade do bloco**:

> â€œTransformar uma imagem mais simples numa representaÃ§Ã£o mais abstrata e menorâ€

---

## ğŸ§± 5. Stack de Convs â€” *Hierarquia de significado*

### Fluxo real

```bash
Conv (bordas)
â†’ Conv (texturas)
â†’ Conv (partes)
â†’ Conv (objetos)
```

Cada Conv **confia** que:

* a anterior jÃ¡ organizou a informaÃ§Ã£o

---

## ğŸ“ 6. Flatten â€” *MudanÃ§a de contrato*

### âœ” Flatten Recebe

```python
(batch, C, H, W)
```

### âœ” Flatten Recebe de

* Ãšltima Conv

### âœ” Flatten Faz

* Converte tensor espacial em vetor

### âœ” Flatten Retorna

```python
(batch, C*H*W)
```

A imagem â€œvira uma lista de nÃºmerosâ€.

Antes

```bash
(batch, 64, 8, 8)
```

Depois

```bash
(batch, 4096)
```

ğŸ‘‰ Aqui acaba a noÃ§Ã£o de espaÃ§o (H e W somem).

### âœ” Flatten Retorna para

* Linear

### âŒ Flatten NÃƒO faz

* Aprender
* Normalizar
* Classificar

ğŸ“Œ **Aqui muda tudo**:
ğŸ‘‰ a rede deixa de ser espacial e vira **vetorial**

---

## ğŸ§® 7. Linear â€” *Combinador global*

### âœ” Linear Recebe

```python
(batch, features)
```

Features = â€œinformaÃ§Ãµes jÃ¡ extraÃ­dasâ€

NÃ£o Ã© mais imagem

Ã‰ um vetor

### âœ” Linear Recebe de

* Flatten
* Outra Linear

### âœ” Linear Faz

* Combina TODAS as features
* Aprende relaÃ§Ãµes globais

### âœ” Linear Retorna

```python
(batch, output_features)
```

SaÃ­da de uma camada Linear intermediÃ¡ria.

RepresentaÃ§Ã£o abstrata

CombinaÃ§Ã£o global das features

```bash
nn.Linear(4096, 128)
```

```bash
â†’ hidden_features = 128
```

### âœ” Linear Retorna para

* Outra Linear
* SaÃ­da

### âŒ Linear NÃƒO faz

* Ver espaÃ§o
* Detectar padrÃµes locais

ğŸ“Œ **Regra mental**:

> Linear sÃ³ entende nÃºmeros, nÃ£o imagens

---

## ğŸ¯ 8. Camada de saÃ­da â€” *Geradora de logits*

### âœ” SaÃ­da Recebe

```python
(batch, hidden_features)
```

RepresentaÃ§Ã£o abstrata

CombinaÃ§Ã£o global das features

```bash
nn.Linear(4096, 128)
â†’ hidden_features = 128
```

### âœ” SaÃ­da Recebe de

* Ãšltima Linear

### âœ” SaÃ­da Faz

* Gera um score por classe

### âœ” SaÃ­da Retorna

```python
(batch, num_classes)
```

### âœ” SaÃ­da Retorna para

* FunÃ§Ã£o de loss

### âŒ SaÃ­da NÃƒO faz

* Softmax
* DecisÃ£o final

---

## ğŸ§  9. Loss â€” *Juiz*

### âœ” Loss Recebe

* logits `(batch, num_classes)`
* labels `(batch)`

### âœ” Loss Faz

* Mede erro
* Gera gradientes

### âŒ Loss NÃƒO faz

* Atualizar pesos

---

## ğŸ”„ 10. Optimizer â€” *Executor*

### âœ” Optimizer Recebe

* Gradientes

### âœ” Optimizer Faz

* Atualiza pesos

---

## ğŸ”— VISÃƒO COMPLETA DO FLUXO

```bash
DataLoader
 â†“
Conv â†’ ReLU â†’ Pool
 â†“
Conv â†’ ReLU â†’ Pool
 â†“
Flatten
 â†“
Linear â†’ ReLU
 â†“
Linear (logits)
 â†“
Loss
 â†“
Backward
 â†“
Optimizer
```

ğŸ”— VISÃƒO FINAL ENCADEADA

```bash
(batch, 3, H, W)          â†’ imagem
â†“
(batch, C1, H, W)        â†’ conv
â†“
(batch, C1, H/2, W/2)    â†’ pool
â†“
(batch, C2, H/2, W/2)    â†’ conv
â†“
(batch, C2, H/4, W/4)    â†’ pool
â†“
(batch, C2*H*W)          â†’ flatten
â†“
(batch, hidden_features) â†’ linear
â†“
(batch, num_classes)     â†’ logits
```

---

## ğŸ§  REGRA DE OURO (GUARDE ISSO)

> **Cada camada sÃ³ conhece o tipo de dado que recebe.
> Ela NÃƒO sabe o que veio antes, nem para onde vai depois.**

## ğŸ§  FRASE PARA GRAVAR

> Enquanto existir H e W, a rede estÃ¡ â€œolhando imagensâ€.
> Quando vira vetor, ela estÃ¡ â€œtomando decisÃµesâ€.

---

## ğŸ§ª EXERCÃCIO (ESSENCIAL)

Responda em voz alta ou escrevendo:

1ï¸âƒ£ O que a `Conv2d` espera receber?
2ï¸âƒ£ O que quebra se vocÃª mandar um vetor para uma Conv?
3ï¸âƒ£ Por que `Flatten` sÃ³ aparece uma vez?
4ï¸âƒ£ Quem decide a classe? A rede ou a loss?
